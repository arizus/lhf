# How Learning from Human Feedback Influences the Lexical Choices of Large Language Models

This repository contains code and data for our paper:  
**"How Learning from Human Feedback Influences the Lexical Choices of Large Language Models"**

## Overview
Large Language Models (LLMs) are known to overuse certain words, such as *delve* and *intricate*. This project investigates whether Learning from Human Feedback (LHF) contributes to this phenomenon. We introduce a method for identifying potentially LHF-induced lexical preferences and critically, we conduct an experimental study to test our hypothesis, showing that LHF indeed influences lexical choices of LLMs.

## Contents
- **Paper:** The manuscript detailing our methodology, results, and conclusions.
- **Code:** The scripts used for our work.
- **Data:** The data analysed in the paper.

## Citation
We are trying to add an anonymous arxiv submission. Please check back again soon. 

## Licence
tbd
